

I am a Ph.D. student at the [School of Information](https://www.ischool.utexas.edu/) at the University of Texas at Austin. I am  co-advised by [Dr. Matt Lease](https://www.ischool.utexas.edu/~ml/) and [Dr. Jessy Li](https://jessyli.com/). I am a part of the [Information Retrieval and Crowdsourcing Lab](http://ir.ischool.utexas.edu/) and associated with the [UT NLP Group](https://www.nlp.utexas.edu/). During my PhD, I have also interned at [The Max-Planck Institute for Informatics](https://www.mpi-inf.mpg.de/home/) in summer 2019 and worked with [Dr. Gerhard Weikum](https://people.mpi-inf.mpg.de/~weikum/). 

Before joining the Ph.D. program, I worked as a Software Engineer in [Microsoft](https://www.microsoft.com/en-in/msidc/default.aspx) and as a Decision Scientist in [Mu Sigma](https://www.mu-sigma.com/). I received my Bachelor of Engineering Degree in Computer Science and Technology from [IIEST, Shibpur](http://www.iiests.ac.in/index.php).

## Research

![Research Intersets](/assets/images/Research.png)

I am interested in the intersection of Natural Language Processing and Human-Computer Interaction. I research in building algorithms and tools that facilitate human-AI partnership. My primary research focuses is based on the broad questions: How explainable AI can help in the human decision-making process where the AI is not perfect? How do we collect data, build such systems and evaluate them? The application area I am interested in is moderating harmful content online. Such harm includes but not limited to misinformation, hate speech, disruptive or obscene content, and unfairness in information access algorithms.

## News

* I will be spending the summer of 2023 at the Amazon Alexa Responsible AI team and I will continue to work on explainable NLP

* Paper on Human-Centered NLP for Fact-Checking is published in a special issue of the IPM (Impact Factor: 6.222) journal [[Arxiv]](https://arxiv.org/abs/2301.03056)

* Paper on Explaining Black-box NLP models with Case-based reasoning is accepted in [ACL 2022](https://www.2022.aclweb.org/). [[arxiv](https://arxiv.org/abs/2204.05426)] [[code](https://github.com/anubrata/ProtoTEx)]

* Paper on Interactive AI for Fact-Checking is accepted in [ACM CHIIR 2022](https://ai.ur.de/chiir2022/)[[arxiv](https://arxiv.org/abs/2202.08901)]

## Talks

* The state of human-centered NLP technology for fact-checking
  * Invited talk at the [Information Processing & Management Conference 2022](https://www.elsevier.com/events/conferences/information-processing-and-management-conference) for our journal [paper](https://arxiv.org/abs/2301.03056)


* ProtoTEx: Explaining Model Decisions with Prototype Tensors [[Video]](https://www.youtube.com/watch?v=QvPdYlsJGrg)
  * [Explainable AI Group](https://twitter.com/XAI_Research), 09/29/2022
  * Research Colloquium, UT Austin, iSchool, 09/20/2022
  * iSchools European Doctoral Seminar Series, 09/16/2022
  * Amazon Science Clarify Team, 05/17/2022
  * NEC Laboratories Europe, 06/09/2022

* You Are What You Tweet: Profiling Users by Past Tweets to Improve Hate Speech Detection. [[Video]](https://youtu.be/kNP9BC3H0D4)
  * Presented on behalf of Prateek Chaudhry and Matthew Lease at the [iConference 2022](https://ischools.org/Short-Research-Papers). 

* ExFacto: An Explainable Fact-Checking Tool [[slides]](https://docs.google.com/presentation/d/1cjGGAtEwjrf8KXWgwJtOqoJGd3WoXjcUGzC49YL28v4/edit?usp=sharing) [[Video]](https://youtu.be/1Ltdoctl8cE)
   * The Knight Research Network (KRN) Demo Day at [Center for Informed Democracy & Social - cybersecurity (IDeaS)](https://www.cmu.edu/ideas-social-cybersecurity/events/krn-tool-demo.html) at Carnegie Mellon University, 10/13/2021

* Commerical Content Moderation and Worker Well-Being. [[Video]](https://youtu.be/4ZIiGIkYdNA)
    * TxHCI - A seminar organized by HCI Researchers across universities in Texas, 10/02/2020
    * Invited talk - Amazon AWS Science, 10/14/2020
    * Invited talk - Amazon Human-in-the-loop (HILL) services team, 10/23/2020
    * [AAAI HCOMP 2020](https://www.humancomputation.com/)

* CobWeb: A Research Prototype for Exploring User Bias in Political Fact-Checking [[Slides](https://docs.google.com/presentation/d/17Px--Lp50Os95QVfuH6auGzdaZReM-CWjuGnDJVQDG8/edit?usp=sharing)]
    *  Presented at the SIGIR 2019 Workshop Fairness, Accountability, Confidentiality, Transparency, and Safety in Information retrieval [[FACTS-IR](https://fate-events.github.io/facts-ir/)]

## Selected Publications

Full list of publications on: ([Google Scholar](https://scholar.google.com/citations?hl=en&user=zVcu-J4AAAAJ))
(\* = equal contribution)

* [The state of human-centered NLP technology for fact-checking](https://doi.org/10.1016/j.ipm.2022.103219) [[Arxiv]](https://arxiv.org/abs/2301.03056)
<br/>**Das, Anubrata**, Houjiang Liu, Venelin Kovatchev, and Matthew Lease. 
<br/>Information processing & management 60, no. 2 (2023): 103219.

* [ProtoTex: Explaining Model Decisions with Prototype Tensors](https://arxiv.org/abs/2204.05426) [[code](https://github.com/anubrata/ProtoTEx)] |[[slides](https://utexas.app.box.com/v/das-acl22-slides)] | [[Talk](https://youtu.be/QvPdYlsJGrg)] | [[Poster](https://drive.google.com/file/d/10i69YGMfj2FxcPTu6NtUuJuhHsD3eIQC/view?usp=sharing)]
<br/> **Anubrata Das**\*, Chitrank Gupta\*, Venelin Kovatchev, Matthew Lease, and Junyi Jessy Li. 
<br/> In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), 2022.

* [The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims](https://arxiv.org/abs/2202.08901)
<br /> Li Shi, Nilavra Bhattacharya, **Anubrata Das**, Matthew Lease, and Jacek Gwizdka. 
<br /> In Proceedings of the 7th ACM SIGIR Conference on Human Information, Interaction and Retrieval (CHIIR), 2022.

* [Fairness in Information Access Systems](https://www.nowpublishers.com/article/Details/INR-079)
<br />Michael D. Ekstrand, **Anubrata Das**, Robin Bruke, Fernando Diaz
<br />[Foundations and Trends in Information Retrieval](https://www.nowpublishers.com/INR), 2022


* [Fast, Accurate, and Healthier: Interactive Blurring Helps Moderators Reduce Exposure to Harmful Content](https://www.ischool.utexas.edu/~ml/papers/das_hcomp20.pdf)
<br />**Anubrata Das**, Brandon Dang, and Matthew Lease
<br /> AAAI Conferenece on Human Computation, 2020 

* [Dataset bias: A case study for visual question answering](https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.7) 
<br />**Anubrata Das**, Samreen Anjum and Danna Gurari
<br />Proceedings of the Association for Information Science and Technology 56, no. 1 (2019): 58-67.

* [CobWeb: A Research Prototype for Exploring User Bias in Political Fact-Checking](https://arxiv.org/pdf/1907.03718.pdf)
<br />**Anubrata Das**, Kunjan Mehta, and Matthew Lease
<br />[FACTS-IR Workshop](https://fate-events.github.io/facts-ir/), SIGIR 2019. [[slides](https://docs.google.com/presentation/d/17Px--Lp50Os95QVfuH6auGzdaZReM-CWjuGnDJVQDG8/edit?usp=sharing)]

* [A Conceptual Framework for Evaluating Fairness in Search](https://arxiv.org/pdf/1907.09328.pdf)
<br />**Anubrata Das** and Matthew Lease
<br />arXiv preprint arXiv:	arXiv:1907.09328 (2019)

* [Interactive information crowdsourcing for disaster management using SMS and Twitter: A research prototype](https://www.iimcal.ac.in/sites/all/files/pdfs/6-casper-iimc.pdf)
<br />**Anubrata Das**, Neeratyoy Mallik, Somprakash Bandyopadhyay, Sipra Das Bit, and Jayanta Basak
<br />IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops) 2016
 
 * [Predicting trends in the twitter social network: A machine learning approach](https://www.researchgate.net/profile/Soumi_Dutta/publication/294482813_Predicting_Trends_in_the_Twitter_Social_Network_A_Machine_Learning_Approach/links/5b14c6bc0f7e9b498108eebe/Predicting-Trends-in-the-Twitter-Social-Network-A-Machine-Learning-Approach.pdf)
<br />**Anubrata Das**, Moumita Roy, Soumi Dutta, Saptarshi Ghosh, and Asit Kumar Das
<br />In International Conference on Swarm, Evolutionary, and Memetic Computing, Springer, Cham, 2014.
