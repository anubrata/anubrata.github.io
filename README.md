
I am a Ph.D. candidate at the [School of Information](https://www.ischool.utexas.edu/) at the University of Texas at Austin. I am  co-advised by [Dr. Matt Lease](https://www.ischool.utexas.edu/~ml/) and [Dr. Jessy Li](https://jessyli.com/). I am a part of the [Laboratory for Artificial Intelligence and Human-Centered Computing (AI&HCC)](https://ai.ischool.utexas.edu/) and associated with the [UT NLP Group](https://www.nlp.utexas.edu/). During my PhD, I have also interned at [Amazon Alexa Responsible AI Research](https://www.amazon.science/), [Cisco Responsible AI Research](https://research.cisco.com/research-projects/rai), and [The Max-Planck Institute for Informatics](https://www.mpi-inf.mpg.de/home/) where I worked with [Dr. Gerhard Weikum](https://people.mpi-inf.mpg.de/~weikum/). 

Before joining the Ph.D. program, I worked as a Software Engineer in [Microsoft](https://www.microsoft.com/en-in/msidc/default.aspx) and as a Decision Scientist in [Mu Sigma](https://www.mu-sigma.com/). I received my Bachelor of Engineering Degree in Computer Science and Technology from [IIEST, Shibpur](http://www.iiests.ac.in/index.php).

**I am on the job market looking for academic or industry postdoc positions for fall 2024.**

## Research

I am interested in the intersection of Natural Language Processing and Human-Computer Interaction, specifically focused on developing NLP technologies that complement the capabilities of human experts. My work centers on three key thrusts of research: 

* **Human-Centered NLP**: How can we identify the needs of the stakeholders for the practical adoption of NLP applications? How can we evaluate if NLP applications are meeting stakeholder needs? How can research in human-centered NLP help push forward basic NLP research? How can we align NLP models to complement human experts in critical fields effectively? [[Preprint]](https://arxiv.org/abs/2308.07213) [[IPM Journal]](https://arxiv.org/abs/2301.03056)

* **Interpretable NLP Models**: How can we build NLP models to help stakeholders understand its inner workings? How can we effectively evaluate interpretable models? How can we use insights from interpretable models to steer generative model outputs? How can we build interpretable models to help promote responsible and productive human-AI partnerships? [[ACL'22]](https://aclanthology.org/2022.acl-long.213/) [[IPM Journal]](https://arxiv.org/abs/2301.03056)

* **Responsible Language Technologies**: How can we detect and mitigate potential harms caused by language technonologies? How can we make these models behave responisbly and not perpetuate societal biases? How can we protect workers who contribute to data-collection for AI? [[FnTIR Journal]](https://www.nowpublishers.com/article/Details/INR-079) [[HCOMP'20]](https://ojs.aaai.org/index.php/HCOMP/article/view/7461) [[ASIS&T'19]](https://doi.org/10.1002/pra2.7)

## News

* I spent Fall 2023 as a research intern at Cisco Responsible AI research team and worked on evaluating interpretable NLP models

* I spent Summer 2023 at the Amazon Alexa Responsible AI team and worked on developing interpretable NLP

* Paper on Human-Centered NLP for Fact-Checking is published in a special issue of the IPM (Impact Factor: 6.222) journal [[Arxiv]](https://arxiv.org/abs/2301.03056)

* Paper on Explaining Black-box NLP models with Case-based reasoning is accepted in [ACL 2022](https://www.2022.aclweb.org/). [[arxiv](https://arxiv.org/abs/2204.05426)] [[code](https://github.com/anubrata/ProtoTEx)]

* Paper on Interactive AI for Fact-Checking is accepted in [ACM CHIIR 2022](https://ai.ur.de/chiir2022/)[[arxiv](https://arxiv.org/abs/2202.08901)]

## Talks

* The state of human-centered NLP technology for fact-checking
  * Invited talk at the [Information Processing & Management Conference 2022](https://www.elsevier.com/events/conferences/information-processing-and-management-conference) for our journal [paper](https://arxiv.org/abs/2301.03056)


* ProtoTEx: Explaining Model Decisions with Prototype Tensors [[Video]](https://www.youtube.com/watch?v=QvPdYlsJGrg)
  * [Explainable AI Group](https://twitter.com/XAI_Research), 09/29/2022
  * Research Colloquium, UT Austin, iSchool, 09/20/2022
  * iSchools European Doctoral Seminar Series, 09/16/2022
  * Amazon Science Clarify Team, 05/17/2022
  * NEC Laboratories Europe, 06/09/2022

* You Are What You Tweet: Profiling Users by Past Tweets to Improve Hate Speech Detection. [[Video]](https://youtu.be/kNP9BC3H0D4)
  * Presented on behalf of Prateek Chaudhry and Matthew Lease at the [iConference 2022](https://ischools.org/Short-Research-Papers). 

* ExFacto: An Explainable Fact-Checking Tool [[slides]](https://docs.google.com/presentation/d/1cjGGAtEwjrf8KXWgwJtOqoJGd3WoXjcUGzC49YL28v4/edit?usp=sharing) [[Video]](https://youtu.be/1Ltdoctl8cE)
   * The Knight Research Network (KRN) Demo Day at [Center for Informed Democracy & Social - cybersecurity (IDeaS)](https://www.cmu.edu/ideas-social-cybersecurity/events/krn-tool-demo.html) at Carnegie Mellon University, 10/13/2021

* Commerical Content Moderation and Worker Well-Being. [[Video]](https://youtu.be/4ZIiGIkYdNA)
    * TxHCI - A seminar organized by HCI Researchers across universities in Texas, 10/02/2020
    * Invited talk - Amazon AWS Science, 10/14/2020
    * Invited talk - Amazon Human-in-the-loop (HILL) services team, 10/23/2020
    * [AAAI HCOMP 2020](https://www.humancomputation.com/)

* CobWeb: A Research Prototype for Exploring User Bias in Political Fact-Checking [[Slides](https://docs.google.com/presentation/d/17Px--Lp50Os95QVfuH6auGzdaZReM-CWjuGnDJVQDG8/edit?usp=sharing)]
    *  Presented at the SIGIR 2019 Workshop Fairness, Accountability, Confidentiality, Transparency, and Safety in Information retrieval [[FACTS-IR](https://fate-events.github.io/facts-ir/)]

## Selected Publications

Full list of publications on: ([Google Scholar](https://scholar.google.com/citations?hl=en&user=zVcu-J4AAAAJ&view_op=list_works&sortby=pubdate))
(\* = equal contribution)

* [Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using Matchmaking for AI](https://arxiv.org/pdf/2308.07213.pdf)
<br/> Houjiang Liu\*, **Anubrata Das\***, Alexander Boltz\*, Didi Zhou, Daisy Pinaroc, Matthew Lease, Min Kyung Lee
<br/> Arxiv Preprint

* [The state of human-centered NLP technology for fact-checking](https://doi.org/10.1016/j.ipm.2022.103219) [[Arxiv]](https://arxiv.org/abs/2301.03056)
<br/>**Anubrata Das**, Houjiang Liu, Venelin Kovatchev, and Matthew Lease. 
<br/>Information processing & management 60, no. 2 (2023): 103219.

* [True or false? Cognitive load when reading COVID-19 news headlines: an eye-tracking study](https://doi.org/10.1145/3576840.3578290)
<br/> Li Shi, Nilavra Bhattacharya, **Anubrata Das**, and Jacek Gwizdka.
<br /> In Proceedings of the 8th ACM SIGIR Conference on Human Information, Interaction and Retrieval (CHIIR), 2023.

* [ProtoTex: Explaining Model Decisions with Prototype Tensors](https://arxiv.org/abs/2204.05426) [[code](https://github.com/anubrata/ProtoTEx)] |[[slides](https://utexas.app.box.com/v/das-acl22-slides)] | [[Talk](https://youtu.be/QvPdYlsJGrg)] | [[Poster](https://drive.google.com/file/d/10i69YGMfj2FxcPTu6NtUuJuhHsD3eIQC/view?usp=sharing)]
<br/> **Anubrata Das**\*, Chitrank Gupta\*, Venelin Kovatchev, Matthew Lease, and Junyi Jessy Li. 
<br/> In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), 2022.

* [The Effects of Interactive AI Design on User Behavior: An Eye-tracking Study of Fact-checking COVID-19 Claims](https://arxiv.org/abs/2202.08901)
<br /> Li Shi, Nilavra Bhattacharya, **Anubrata Das**, Matthew Lease, and Jacek Gwizdka. 
<br /> In Proceedings of the 7th ACM SIGIR Conference on Human Information, Interaction and Retrieval (CHIIR), 2022.

* [Fairness in Information Access Systems](https://www.nowpublishers.com/article/Details/INR-079)
<br />Michael D. Ekstrand, **Anubrata Das**, Robin Bruke, Fernando Diaz
<br />[Foundations and Trends in Information Retrieval](https://www.nowpublishers.com/INR), 2022


* [Fast, Accurate, and Healthier: Interactive Blurring Helps Moderators Reduce Exposure to Harmful Content](https://www.ischool.utexas.edu/~ml/papers/das_hcomp20.pdf)
<br />**Anubrata Das**, Brandon Dang, and Matthew Lease
<br /> AAAI Conferenece on Human Computation, 2020 

* [Dataset bias: A case study for visual question answering](https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/pra2.7) 
<br />**Anubrata Das**, Samreen Anjum and Danna Gurari
<br />Proceedings of the Association for Information Science and Technology 56, no. 1 (2019): 58-67.

* [CobWeb: A Research Prototype for Exploring User Bias in Political Fact-Checking](https://arxiv.org/pdf/1907.03718.pdf)
<br />**Anubrata Das**, Kunjan Mehta, and Matthew Lease
<br />[FACTS-IR Workshop](https://fate-events.github.io/facts-ir/), SIGIR 2019. [[slides](https://docs.google.com/presentation/d/17Px--Lp50Os95QVfuH6auGzdaZReM-CWjuGnDJVQDG8/edit?usp=sharing)]

* [A Conceptual Framework for Evaluating Fairness in Search](https://arxiv.org/pdf/1907.09328.pdf)
<br />**Anubrata Das** and Matthew Lease
<br />arXiv preprint arXiv:	arXiv:1907.09328 (2019)

* [Interactive information crowdsourcing for disaster management using SMS and Twitter: A research prototype](https://www.iimcal.ac.in/sites/all/files/pdfs/6-casper-iimc.pdf)
<br />**Anubrata Das**, Neeratyoy Mallik, Somprakash Bandyopadhyay, Sipra Das Bit, and Jayanta Basak
<br />IEEE International Conference on Pervasive Computing and Communication Workshops (PerCom Workshops) 2016
 
 * [Predicting trends in the twitter social network: A machine learning approach](https://www.researchgate.net/profile/Soumi_Dutta/publication/294482813_Predicting_Trends_in_the_Twitter_Social_Network_A_Machine_Learning_Approach/links/5b14c6bc0f7e9b498108eebe/Predicting-Trends-in-the-Twitter-Social-Network-A-Machine-Learning-Approach.pdf)
<br />**Anubrata Das**, Moumita Roy, Soumi Dutta, Saptarshi Ghosh, and Asit Kumar Das
<br />In International Conference on Swarm, Evolutionary, and Memetic Computing, Springer, Cham, 2014.
